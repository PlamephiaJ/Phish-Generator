<!DOCTYPE html>

<html lang="en" data-content_root="../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Quickstart: BERT Pre-trained Model Fine-tuning on WAF Dataset &#8212; Phish-Generator 0.0.1 documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=5ecbeea2" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css?v=b08954a9" />
    <link rel="stylesheet" type="text/css" href="../_static/alabaster.css?v=27fed22d" />
    <script src="../_static/documentation_options.js?v=d45e8c67"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="codestyle" href="codestyle.html" />
    <link rel="prev" title="Installation" href="install.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="quickstart-bert-pre-trained-model-fine-tuning-on-waf-dataset">
<span id="quickstart"></span><h1>Quickstart: BERT Pre-trained Model Fine-tuning on WAF Dataset<a class="headerlink" href="#quickstart-bert-pre-trained-model-fine-tuning-on-waf-dataset" title="Link to this heading">¶</a></h1>
<p>This guide demonstrates how to fine-tune a BERT pre-trained model on a Web Application Firewall (WAF) dataset for malicious request classification.</p>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Link to this heading">¶</a></h2>
<p>In this example, we fine-tune the compact <code class="docutils literal notranslate"><span class="pre">bert_uncased_L-2_H-128_A-2</span></code> model from Hugging Face to classify HTTP requests as malicious or benign using a WAF dataset. This process leverages the <code class="docutils literal notranslate"><span class="pre">smf</span></code> framework for efficient model training. For background, refer to <a class="footnote-reference brackets" href="#id2" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>.</p>
<p>Prerequisites:</p>
<ul class="simple">
<li><p>The latest version of the <code class="docutils literal notranslate"><span class="pre">smf</span></code> framework and its dependencies, installed per the <a class="reference external" href="https://github.com/smf/smf-install">installation guide</a>. Using the provided Docker image is recommended.</p></li>
<li><p>A GPU with at least 16 GB HBM (e.g., NVIDIA RTX3090 or RTX4090).</p></li>
<li><p>Python 3.9+ and Conda installed.</p></li>
<li><p>Access to the WAF dataset repository on CodeHub.</p></li>
</ul>
</section>
<section id="dataset-introduction">
<h2>Dataset Introduction<a class="headerlink" href="#dataset-introduction" title="Link to this heading">¶</a></h2>
<p>The WAF dataset contains HTTP request logs labeled as malicious or benign, designed for training models to detect web-based attacks (e.g., SQL injection, XSS). It includes features such as URL paths, headers, and payloads, preprocessed into a format suitable for BERT’s input pipeline. The dataset is hosted on the CodeHub dataset repository and consists of approximately 50,000 samples, split into 80% training and 20% validation sets.</p>
</section>
<section id="step-1-prepare-the-dataset">
<h2>Step 1: Prepare the Dataset<a class="headerlink" href="#step-1-prepare-the-dataset" title="Link to this heading">¶</a></h2>
<p>Clone the WAF dataset from the CodeHub repository and initialize submodules.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://codehub.example.com/waf-dataset.git
<span class="nb">cd</span><span class="w"> </span>waf-dataset
git<span class="w"> </span>submodule<span class="w"> </span>init
git<span class="w"> </span>submodule<span class="w"> </span>update
</pre></div>
</div>
<p>The dataset will be downloaded to the <code class="docutils literal notranslate"><span class="pre">data/</span></code> directory in CSV format, with columns for request text and labels (0 for benign, 1 for malicious).</p>
</section>
<section id="step-2-download-the-base-model">
<h2>Step 2: Download the Base Model<a class="headerlink" href="#step-2-download-the-base-model" title="Link to this heading">¶</a></h2>
<p>Download the pre-trained <code class="docutils literal notranslate"><span class="pre">bert_uncased_L-2_H-128_A-2</span></code> model from Hugging Face.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mkdir<span class="w"> </span>-p<span class="w"> </span>models
<span class="nb">cd</span><span class="w"> </span>models
git<span class="w"> </span>lfs<span class="w"> </span>install
git<span class="w"> </span>clone<span class="w"> </span>https://huggingface.co/google/bert_uncased_L-2_H-128_A-2
</pre></div>
</div>
<p>The model weights and configuration will be stored in <code class="docutils literal notranslate"><span class="pre">models/bert_uncased_L-2_H-128_A-2/</span></code>.</p>
</section>
<section id="step-3-fine-tune-the-model">
<h2>Step 3: Fine-tune the Model<a class="headerlink" href="#step-3-fine-tune-the-model" title="Link to this heading">¶</a></h2>
<p>Fine-tune the model using a configuration file tailored for the WAF dataset.</p>
<ol class="arabic simple">
<li><p>Ensure the <code class="docutils literal notranslate"><span class="pre">smf_env</span></code> Conda environment is activated:</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>conda<span class="w"> </span>activate<span class="w"> </span>smf_env
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>Navigate to the <code class="docutils literal notranslate"><span class="pre">smf</span></code> source directory and run the training script with the provided configuration:</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>smf/src
python<span class="w"> </span>main.py<span class="w"> </span>--config<span class="w"> </span>ai_waf/bert_L2H128A2.yaml
</pre></div>
</div>
<p>Sample configuration file (<code class="docutils literal notranslate"><span class="pre">ai_waf/bert_L2H128A2.yaml</span></code>):</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">model</span><span class="p">:</span>
<span class="w">  </span><span class="nt">pretrained_path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">../models/bert_uncased_L-2_H-128_A-2</span>
<span class="w">  </span><span class="nt">num_labels</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2</span>
<span class="nt">dataset</span><span class="p">:</span>
<span class="w">  </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">../../waf-dataset/data</span>
<span class="w">  </span><span class="nt">train_split</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">train.csv</span>
<span class="w">  </span><span class="nt">val_split</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">val.csv</span>
<span class="w">  </span><span class="nt">max_length</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">128</span>
<span class="nt">training</span><span class="p">:</span>
<span class="w">  </span><span class="nt">batch_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">32</span>
<span class="w">  </span><span class="nt">learning_rate</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2e-5</span>
<span class="w">  </span><span class="nt">epochs</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">3</span>
<span class="w">  </span><span class="nt">optimizer</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">adamw</span>
<span class="w">  </span><span class="nt">scheduler</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">linear</span>
<span class="nt">output</span><span class="p">:</span>
<span class="w">  </span><span class="nt">save_dir</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">../outputs/waf_finetuned</span>
<span class="w">  </span><span class="nt">log_interval</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">100</span>
</pre></div>
</div>
<p>This configuration specifies the model path, dataset details, and training hyperparameters. Adjust paths as needed based on your directory structure.</p>
</section>
<section id="step-4-evaluate-the-model">
<h2>Step 4: Evaluate the Model<a class="headerlink" href="#step-4-evaluate-the-model" title="Link to this heading">¶</a></h2>
<p>After training, evaluate the model on the validation set:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>evaluate.py<span class="w"> </span>--model<span class="w"> </span>../outputs/waf_finetuned<span class="w"> </span>--data<span class="w"> </span>../../waf-dataset/data/val.csv
</pre></div>
</div>
<p>The script outputs metrics such as accuracy, precision, recall, and F1-score.</p>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading">¶</a></h2>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="id2" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>Well-Read Students Learn Better: On the Importance of Pre-training Compact Models (<a class="reference external" href="https://arxiv.org/abs/1908.08962">https://arxiv.org/abs/1908.08962</a>). This paper highlights the benefits of pre-training compact models like BERT for downstream tasks.</p>
</aside>
</aside>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../index.html">Phish-Generator</a></h1>









<search id="searchbox" style="display: none" role="search">
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" placeholder="Search"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script><h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">GETTING STARTED</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="install.html#pre-requisites">Pre-requisites</a></li>
<li class="toctree-l1"><a class="reference internal" href="install.html#install-dependencies">Install Dependencies</a></li>
<li class="toctree-l1"><a class="reference internal" href="install.html#post-installation">Post-installation</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Quickstart: BERT Pre-trained Model Fine-tuning on WAF Dataset</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="#dataset-introduction">Dataset Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="#step-1-prepare-the-dataset">Step 1: Prepare the Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="#step-2-download-the-base-model">Step 2: Download the Base Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="#step-3-fine-tune-the-model">Step 3: Fine-tune the Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="#step-4-evaluate-the-model">Step 4: Evaluate the Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="codestyle.html">codestyle</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
      <li>Previous: <a href="install.html" title="previous chapter">Installation</a></li>
      <li>Next: <a href="codestyle.html" title="next chapter">codestyle</a></li>
  </ul></li>
</ul>
</div>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2025, CNLab_Victoria.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 8.2.3</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 1.0.0</a>
      
      |
      <a href="../_sources/start/quickstart.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>